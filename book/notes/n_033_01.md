In the mid 1970's, C was designed to support a range of computer generations.  The PDP-11
was a common "previous-generation" computer that has less memory and so variable sizes were kept small.
The more modern computers in this chart had a bit more memory and so could afford to have slightly larger
sizes.  

The idea of "natural" size is the size that could be loaded, computed, and stored in a single
machine language instruction.  You knew as a programmer that when you used `int`, the machine code you would
generate would not need to include extra instructions for a simple line of code like `x = x + 1;`.

Modern `int` values in C are 32-bits long and `long` values are 64-bits long.  Even though modern computers can
do 64-bit computations in a single instruction, using the shorter `int` type when appropriate
can save on memory storage and memory bandwidth when using `int` values.

Interestingly the length of a 32-bit `int` leads to a UNIX/C problem with dates that is called the
"Year 2038 Problem".  A common way to represent time in UNIX/C programs was as a 32-bit integer
in the number of seconds since 1-Jan-1970.  It was quick and easy to compare, add or subtract these
second counter dates in code and even in databases.   But the number of seconds since 1-Jan-1970
will overflow a 32-bit number on 19-Jan-2038.   By now, in order to avoid problems most systems
have converted to storing these "number of seconds" values in `long` (64-bit) values.   Which gives
us almost 300 billion years until we need to worry about overflowing timer counters again.

Back when C was developed we had two different character sets and two different `char`
variable lengths.  The world generally standardized on the `ASCII` character set for the core western
characters and Unicode / UTF-8 to represent all characters in all languages worldwide.  But that is
a story for another time.  For now, just think of the `char` type as also a `byte` type, is 8-bits in length
and can store ASCII.  Modern languages like `Python` or `Java` have excellent support
for wide character sets.  In our historical look at C - we will not cover wide or multi-byte characters.

Also if you look at the `float` and `double` types, you see different bit-sizes.  Even worse, each of
these computers did floating point computation using slightly different hardware implementations and
the same code run on different computers would give *slightly different* results and have unpredictable behavior
on overflow, underflow and other extraordinary floating point operations.  This was solves by the introduction
of the IEEE 754 (1985) floating point format standard - which standardized both the length of `float` and `double`
but insured that the same set of floating point calculations would produce the *exact same result* on different
processors.

